{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "     \n",
    "\n",
    "# The base URL of the website we want to crawl\n",
    "base_url = 'https://www.mql5.com/en/signals/mt5/list/page'\n",
    "\n",
    "# The list of pages we want to crawl // Useless code \n",
    "page_urls = ['1']\n",
    "\n",
    "# The CSV file we want to write the data to\n",
    "csv_file = open('signal_list.csv', 'w', newline='')\n",
    "writer = csv.writer(csv_file)\n",
    "writer.writerow(['Signal Id'])\n",
    "\n",
    "# Loop over each page URL and extract the Signal ID data\n",
    "for page_url in page_urls:\n",
    "    # Construct the full URL for the page\n",
    "    url = base_url + page_url\n",
    "\n",
    "    # Make a GET request to the page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all the element has id \n",
    "    ids = soup.find_all('div', class_='button button_white-and-green')\n",
    "\n",
    "    #extrac the id\n",
    "    for id in ids:\n",
    "        name = id.get(\"data-id\")\n",
    "        # Write the data to the CSV file\n",
    "        writer.writerow([name])\n",
    "\n",
    "# Close the CSV file\n",
    "csv_file.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
