{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from urllib.parse import quote\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from selenium.webdriver.support import expected_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "def dateval(link):\n",
    "# Extract the 8-digit date using a regular expression\n",
    "    date_pattern = r'(\\d{8})\\d{9}\\.htm$'\n",
    "    match = re.search(date_pattern, link)\n",
    "\n",
    "    if match:\n",
    "        extracted_date = match.group(1)\n",
    "        print(\"Extracted Date:\", extracted_date)\n",
    "    else:\n",
    "        print(\"Date not found in the link.\")\n",
    "    try:\n",
    "        # Parse the cleaned time string using dateutil.parser\n",
    "        parsed_time = parser.parse(extracted_date)\n",
    "\n",
    "        # Extract the date component\n",
    "        date_only = parsed_time.date()\n",
    "    except ValueError:\n",
    "        print(\"Invalid time format. of: \\n\")\n",
    "    if(date_only.year >2020):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the web driver (e.g., for Chrome)\n",
    "\n",
    "\n",
    "def many_crawler(keyword, searchbase,timecounter):\n",
    "    outputcsv = './tuoitre/'+'tuoitre_'+ str(keyword)+ '.csv'\n",
    "    geturl = searchbase + keyword\n",
    "    driver = webdriver.Edge()\n",
    "    driver.get(geturl)\n",
    "    start_time = time.time()\n",
    "    max_runtime = timecounter\n",
    "    timeflag=True\n",
    "    def clicker():\n",
    "        view_more_link = driver.find_element(By.XPATH,\"//a[@class='view-more']\")\n",
    "        driver.execute_script(\"arguments[0].click();\", view_more_link)\n",
    "        view_more_link.click()\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight-600);\")\n",
    "        try:\n",
    "            clicker()\n",
    "            None\n",
    "        except ElementNotInteractableException:\n",
    "            print(\"not interact\")\n",
    "        except ElementClickInterceptedException:\n",
    "            print(\"not click\")\n",
    "        except NoSuchElementException:\n",
    "            print(\"not such\")\n",
    "        except StaleElementReferenceException:\n",
    "            break\n",
    "        else:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight-600);\")\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        if elapsed_time >= max_runtime:\n",
    "            break\n",
    "        \n",
    "    page_source = driver.page_source\n",
    "    driver.quit()\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    links = soup.find_all('a', class_='box-category-link-title')\n",
    "    with open(outputcsv,'w', encoding='utf-8',newline='') as outputfile:\n",
    "        csv_writer = csv.writer(outputfile)\n",
    "        csv_writer.writerow(['Title', 'URL'])\n",
    "        for link in links:\n",
    "            if(str(link).__contains__(\"c.eclick.vn\")==False):\n",
    "                title = link.get('title')\n",
    "                url = link.get('href')\n",
    "                csv_writer.writerow([title, url])\n",
    "                continue\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n",
      "not interact\n"
     ]
    }
   ],
   "source": [
    "# List of keywords\n",
    "keywords = [\n",
    "    \"trí tuệ nhân tạo\"\n",
    "]\n",
    "import urllib.parse\n",
    "\n",
    "searchbase = \"https://tuoitre.vn/tim-kiem.htm?keywords=\"\n",
    "# Printing the list of Python keywords\n",
    "encoded_keywords=[]\n",
    "for text in keywords:\n",
    "    encoded_string = urllib.parse.quote(text)\n",
    "    many_crawler(keyword=text, searchbase=searchbase,timecounter=(50))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
